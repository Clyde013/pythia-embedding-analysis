{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a67a5c-35b2-454c-bcae-7ae8769bfdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.2.1+cu118)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.17.1+cu118)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.11/site-packages (2.2.1+cu118)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.11/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.11/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.11/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /opt/conda/lib/python3.11/site-packages (from torch) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.11/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.11/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.11/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.11/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba368a02-603e-4155-be2d-312b7f726563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.39.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac09efa-412f-4721-9017-a52d675deaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3af4cd47-88a4-4903-bf57-9521bf11b15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87eb86ea-d861-4958-99a9-efdaf3bb03a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>members</th>\n",
       "      <th>a</th>\n",
       "      <th>n</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.22 caliber</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.22 calibre</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.22-caliber</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.22-calibre</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       members  a  n  r  s  v\n",
       "0          .22  0  1  0  0  0\n",
       "1  .22 caliber  1  0  0  0  0\n",
       "2  .22 calibre  1  0  0  0  0\n",
       "3  .22-caliber  1  0  0  0  0\n",
       "4  .22-calibre  1  0  0  0  0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://huggingface.co/datasets/segyges/openwordnet-categoricals/resolve/main/openwordnet-categoricals.csv\", keep_default_na=False)\n",
    "df_pos = df[['members', 'a', 'n', 'r', 's', 'v']]\n",
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76520d5-34db-4fd8-8986-b01adaa3015b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "members    0\n",
       "a          0\n",
       "n          0\n",
       "r          0\n",
       "s          0\n",
       "v          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ec1c02c-6506-4569-a002-b65e2b5f2a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a  n  r  s  v\n",
       "0  1  0  0  0    116704\n",
       "   0  0  1  0     11366\n",
       "         0  1      7340\n",
       "1  0  0  0  0      5275\n",
       "0  0  1  0  0      3901\n",
       "   1  0  0  1      3701\n",
       "1  0  0  1  0      1442\n",
       "0  1  0  1  0      1230\n",
       "1  1  0  0  0      1007\n",
       "         1  0       343\n",
       "0  0  1  1  0       222\n",
       "   1  0  1  1       220\n",
       "   0  0  1  1       103\n",
       "   1  1  1  0        70\n",
       "1  1  0  1  1        68\n",
       "0  1  1  0  0        65\n",
       "1  0  1  1  0        43\n",
       "         0  0        37\n",
       "   1  1  1  1        34\n",
       "   0  0  1  1        30\n",
       "   1  0  0  1        30\n",
       "      1  1  0        30\n",
       "         0  0        22\n",
       "   0  0  0  1        21\n",
       "0  1  1  1  1        18\n",
       "         0  1        18\n",
       "1  0  1  1  1        11\n",
       "0  0  1  0  1         6\n",
       "         1  1         3\n",
       "1  1  1  0  1         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos[['a', 'n', 'r', 's', 'v']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c8a667-e2b2-4f69-b219-3707d4ef0247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/pythia-12b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e3a02a-80dd-49d5-8bbc-0bfb0a48b2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dog', 'ĠDog', 'Ġendogenous', 'dog', 'Ġdog', 'Ġdogs', 'ĠDogs']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in tokenizer.vocab.keys() if \"dog\" in key.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81d58107-d26c-41f9-8f9d-6d5a2cda2f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68, 288, 288, 100, 288, 288, 288]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(key[0]) for key in tokenizer.vocab.keys() if \"dog\" in key.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e777149a-b63f-4026-a31c-3f3267cc33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(words, embeddings, tokenizer):\n",
    "  X, y, terms = [], [], []\n",
    "  for i, row in words.iterrows():\n",
    "    lowercased = row['members'].lower()\n",
    "    capitalized = row['members'].capitalize()\n",
    "    leading_space = chr(288)\n",
    "    options = [lowercased, capitalized, leading_space + lowercased, leading_space + capitalized]\n",
    "    for option in options:\n",
    "      ids = tokenizer(option, add_special_tokens=False, padding=False)['input_ids']\n",
    "      if len(ids) == 1:\n",
    "        embedding_idx = torch.tensor(ids, device=device)[0]\n",
    "        token = embeddings[embedding_idx]\n",
    "        X.append(token)\n",
    "        y.append(row.drop('members').values.astype(int))\n",
    "        terms.append(option)\n",
    "  return X, y, terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de60f2c-4c3a-4cf8-bbb5-d1432e413fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_preds(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Takes in (y_test, y_pred) and outputs precision, recall, f1, average precision, average recall, ROC_AUC and PR_AUC.\n",
    "    But Gyges ran out of steam so Clyde here taking over to do what i do best. ctrl+c ctrl+v code and take credit.\n",
    "    :)\n",
    "    \"\"\"\n",
    "\n",
    "    n_classes = 5\n",
    "\n",
    "    # Convert y_test and y_pred to numpy arrays if they are not already\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred_onehot = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "    # Average precision score for all classes\n",
    "    micro_precision = average_precision_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Calculate average precision score for each class separately\n",
    "    classwise_avg_precision = average_precision_score(y_test, y_pred, average=None)\n",
    "\n",
    "    # Calculate recall score for each class separately\n",
    "    classwise_recall = recall_score(y_test, y_pred_onehot, average=None)\n",
    "\n",
    "    # Calculate micro-averaged recall\n",
    "    micro_recall = recall_score(y_test, y_pred_onehot, average='micro')\n",
    "\n",
    "    # PR AUC + ROC AUC\n",
    "    # Calculate Precision-Recall curve and area under the curve (PR AUC) for each class separately\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    pr_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):  # Assuming n_classes is the number of classes\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_pred[:, i])\n",
    "        pr_auc[i] = auc(recall[i], precision[i])\n",
    "\n",
    "    # Calculate ROC curve and area under the curve (ROC AUC) for each class separately\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):  # Assuming n_classes is the number of classes\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = roc_auc_score(y_test[:, i], y_pred[:, i])\n",
    "\n",
    "    df_dict = {\"avg precision per class\": list(classwise_avg_precision),\n",
    "               \"avg recall per class\": list(classwise_recall),\n",
    "               \"PR AUC\": list(pr_auc.values()),\n",
    "               \"ROC AUC\": list(roc_auc.values()),\n",
    "               \"micro precision\": [micro_precision],\n",
    "               \"micro recall\": [micro_recall]}\n",
    "    return(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "360b3b1b-de12-4de0-b5d6-c8ef8c9bd46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab2c8f6b-ae72-43d4-8aa5-90b5133014e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just testing that this works a few times to work out any oddities\n",
    "# count = 0\n",
    "# for key, value in data.items():\n",
    "#     print(key)\n",
    "#     print(value.shape)\n",
    "#     count += 1\n",
    "#     if count == 5:\n",
    "#         break\n",
    "\n",
    "#     X, y, terms = get_targets(df_pos, value, tokenizer)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1337)\n",
    "\n",
    "#     classifier = MultiOutputClassifier(make_pipeline(StandardScaler(), SVC(kernel='sigmoid', probability=True)))\n",
    "#     y_pred_proba = classifier.fit(X_train, y_train).predict_proba(X_test)\n",
    "#     # Bunch of numpy fiddling to get the output of predict_proba into the correct format\n",
    "#     output_array = np.stack(y_pred_proba)\n",
    "#     output_transposed = np.transpose(output_array, (1, 0, 2))\n",
    "#     reshaped_array = output_transposed.reshape(output_transposed.shape[0], -1)\n",
    "#     positive_probs = reshaped_array[:, 1::2]\n",
    "#     results = eval_preds(y_test, positive_probs)\n",
    "#     print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e788fe-cd37-4f5a-a24e-40f15233a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    " './pythia-12b-weights/embed_only_0-29000.pkl',\n",
    " './pythia-12b-weights/embed_only_30000-69000.pkl',\n",
    " './pythia-12b-weights/embed_only_70000-109000.pkl',\n",
    " './pythia-12b-weights/embed_only_110000-143000.pkl'\n",
    "]\n",
    "\n",
    "output = {}\n",
    "for filename in files:\n",
    "    with open(filename, 'rb') as in_file:\n",
    "        data = pickle.load(in_file)\n",
    "        for key, value in data.items():\n",
    "            X, y, terms = get_targets(df_pos, value, tokenizer)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1337)\n",
    "        \n",
    "            classifier = MultiOutputClassifier(make_pipeline(StandardScaler(), SVC(kernel='sigmoid', probability=True)))\n",
    "            y_pred_proba = classifier.fit(X_train, y_train).predict_proba(X_test)\n",
    "            # Bunch of numpy fiddling to get the output of predict_proba into the correct format\n",
    "            output_array = np.stack(y_pred_proba)\n",
    "            output_transposed = np.transpose(output_array, (1, 0, 2))\n",
    "            reshaped_array = output_transposed.reshape(output_transposed.shape[0], -1)\n",
    "            positive_probs = reshaped_array[:, 1::2]\n",
    "            results = eval_preds(y_test, positive_probs)\n",
    "            all_results[key] = results\n",
    "            # Save to disk\n",
    "            serialized = json.dumps(all_results)\n",
    "            with open('checkpointed_data.json', 'w') as out_file:\n",
    "                out_file.write(serialized)\n",
    "        del(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
